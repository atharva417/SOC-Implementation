{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One_Shot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jbUMPrEuqJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7212018-6659-41c9-d328-8fb69cc1808f"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1uMmge-vbB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ed7348d3-6466-44db-ea05-041bef277e23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdNfNaEcvb7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cb7748a-a5c7-4c66-bf9a-809790b61bec"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from scipy.misc import imread\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy.random as rng"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7EmXOj0s8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/content/drive/My Drive/SOC_OneShot/train'\n",
        "val_path = '/content/drive/My Drive/SOC_OneShot/val'\n",
        "model_path = '/content/drive/My Drive/SOC_OneShot/weights/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtCkFK2Nv5IE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadimg(path):\n",
        "  X = []\n",
        "  y = []\n",
        "  curr_y = 0\n",
        "  classes = {}\n",
        "  for letter in os.listdir(path):\n",
        "    category_images=[]\n",
        "    letter_path = os.path.join(path, letter)\n",
        "                \n",
        "    for filename in os.listdir(letter_path):\n",
        "      image_path = os.path.join(letter_path, filename)\n",
        "      image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "      category_images.append(image)\n",
        "      y.append(curr_y)\n",
        "    try:\n",
        "      X.append(np.stack(category_images))\n",
        "    except ValueError as e:\n",
        "      print(e)\n",
        "      print(\"error - category_images:\", category_images)\n",
        "    curr_y += 1\n",
        "    classes[letter] = curr_y - 1\n",
        "  y = np.vstack(y)\n",
        "  X = np.stack(X)\n",
        "  return X,y,classes"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNyJVuWh5PmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y, classes = loadimg(train_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCvsUfIe5fG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1cea4b8-f882-4aea-bb14-bf44390c4b2d"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 2, 200, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XjllqIo5kJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74d69ce1-e725-4b01-ab6f-2b013e9b5909"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Be3cascoJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d865e15-2bec-44e1-fb48-63277e93126a"
      },
      "source": [
        "classes.keys()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['I', 'R', 'O', 'SPACE', 'Y', 'DEL', 'K', 'L', 'N', 'W', 'B', 'F', 'D', 'A', 'C'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gacdHbijMJQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xval, yval, classesval = loadimg(val_path)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHHOLNgNGEML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights(shape, name=None, dtype = float):\n",
        "    \"\"\"\n",
        "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Ybw5G8GEu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_bias(shape, name=None, dtype = float):\n",
        "    \"\"\"\n",
        "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4HNS-gaEHaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_model(input_shape):\n",
        "    \"\"\"\n",
        "        Model architecture\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input(input_shape)\n",
        "    right_input = Input(input_shape)\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
        "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(128, (7,7), activation='relu',\n",
        "                     kernel_initializer=initialize_weights,\n",
        "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
        "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
        "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
        "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='sigmoid',\n",
        "                   kernel_regularizer=l2(1e-3),\n",
        "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ul4QIWnEf4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0087e09c-9e7a-46f2-da7d-167bdadd1754"
      },
      "source": [
        "model = get_siamese_model((200, 200, 1))\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 200, 200, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           (None, 200, 200, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       (None, 4096)         27675584    input_9[0][0]                    \n",
            "                                                                 input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 4096)         0           sequential_5[1][0]               \n",
            "                                                                 sequential_5[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            4097        lambda_2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 27,679,681\n",
            "Trainable params: 27,679,681\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6eIuIj7EfZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(lr = 0.00006)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4VcJS265v83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(X,classes,batch_size,s=\"train\"):\n",
        "    \"\"\"\n",
        "    Create batch of n pairs, half same class, half different class\n",
        "    \"\"\"\n",
        "    if s == 'train':\n",
        "        X = X\n",
        "        categories = classes\n",
        "    else:\n",
        "        X = Xval\n",
        "        categories = classesval\n",
        "    n_classes, n_examples, w, h = X.shape\n",
        "    \n",
        "    # randomly sample several classes to use in the batch\n",
        "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
        "    \n",
        "    # initialize 2 empty arrays for the input image batch\n",
        "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
        "    \n",
        "    # initialize vector for the targets\n",
        "    targets=np.zeros((batch_size,))\n",
        "    \n",
        "    # make one half of it '1's, so 2nd half of batch has same class\n",
        "    targets[batch_size//2:] = 1\n",
        "    for i in range(batch_size):\n",
        "        category = categories[i]\n",
        "        idx_1 = rng.randint(0, n_examples)\n",
        "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
        "        idx_2 = rng.randint(0, n_examples)\n",
        "        \n",
        "        # pick images of same class for 1st half, different for 2nd\n",
        "        if i >= batch_size // 2:\n",
        "            category_2 = category  \n",
        "        else: \n",
        "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
        "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
        "        \n",
        "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
        "    \n",
        "    return pairs, targets"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaxJxDMqDxvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(batch_size, s=\"train\"):\n",
        "    \"\"\"\n",
        "    a generator for batches, so model.fit_generator can be used.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        pairs, targets = get_batch(batch_size,s)\n",
        "        yield (pairs, targets)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBJebrY1EgC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_oneshot_task(N, s=\"val\", language=None):\n",
        "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
        "    Xval, yval, classesval = loadimg(val_path)\n",
        "    if s == 'train':\n",
        "        X = X\n",
        "        categories = classes\n",
        "    else:\n",
        "        X = Xval\n",
        "        categories = classesval\n",
        "    n_classes, n_examples, w, h = X.shape\n",
        "    \n",
        "    indices = rng.randint(0, n_examples,size=(N,))\n",
        "    if language is not None: # if language is specified, select characters for that language\n",
        "        low, high = categories[language]\n",
        "        if N > high - low:\n",
        "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
        "        categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
        "\n",
        "    else: # if no language specified just pick a bunch of random letters\n",
        "        categories = rng.choice(range(n_classes),size=(N,),replace=False)           \n",
        "    \n",
        "    true_category = categories[0]\n",
        "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
        "    test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
        "    support_set = X[categories,indices,:,:]\n",
        "    support_set[0,:,:] = X[true_category,ex2]\n",
        "    support_set = support_set.reshape(N, w, h,1)\n",
        "    targets = np.zeros((N,))\n",
        "    targets[0] = 1\n",
        "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "    pairs = [test_image,support_set]\n",
        "    return pairs, targets\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFsSr905EgMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
        "  \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "  n_correct = 0\n",
        "  if verbose:\n",
        "      print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
        "  for i in range(k):\n",
        "      inputs, targets = make_oneshot_task(N,s)\n",
        "      probs = model.predict(inputs)\n",
        "      if np.argmax(probs) == np.argmax(targets):\n",
        "          n_correct+=1\n",
        "  percent_correct = (100.0 * n_correct / k)\n",
        "  if verbose:\n",
        "      print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
        "  return percent_correct"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-8YiJoXEgW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper parameters\n",
        "evaluate_every = 100 # interval for evaluating on one-shot tasks\n",
        "batch_size = 2\n",
        "n_iter = 5000 # No. of training iterations\n",
        "N_way = 9 # how many classes for testing one-shot tasks\n",
        "n_val = 100 # how many one-shot tasks to validate on\n",
        "best = -1"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKmaB6KmEgrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f789ad1-55d2-47e8-e6eb-e8caa12ba47e"
      },
      "source": [
        "t_start = time.time()\n",
        "for i in range(1, n_iter+1):\n",
        "  (inputs,targets) = get_batch(X,classes,batch_size)\n",
        "  loss = model.train_on_batch(inputs, targets)\n",
        "  if i % evaluate_every == 0:\n",
        "    print(\"\\n ------------- \\n\")\n",
        "    print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
        "    print(\"Train Loss: {0}\".format(loss)) \n",
        "    val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
        "    model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
        "    if val_acc >= best:\n",
        "      print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "      best = val_acc"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ------------- \n",
            "\n",
            "Time for 100 iterations: 0.13234188159306845 mins\n",
            "Train Loss: 0.37487107515335083\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 93.0% 9 way one-shot learning accuracy \n",
            "\n",
            "Current best: 93.0, previous best: -1\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 200 iterations: 0.48914155960083006 mins\n",
            "Train Loss: 0.617706298828125\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 300 iterations: 0.8443007628122966 mins\n",
            "Train Loss: 0.593525767326355\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 400 iterations: 1.2416121204694113 mins\n",
            "Train Loss: 0.5158249139785767\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 500 iterations: 1.909624163309733 mins\n",
            "Train Loss: 0.5147621035575867\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 600 iterations: 2.2650404294331867 mins\n",
            "Train Loss: 0.4638809859752655\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 87.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 700 iterations: 2.6238504091898602 mins\n",
            "Train Loss: 0.47635096311569214\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 800 iterations: 2.977455472946167 mins\n",
            "Train Loss: 0.3152725100517273\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 900 iterations: 3.3283525069554645 mins\n",
            "Train Loss: 0.4710049629211426\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1000 iterations: 3.697572962443034 mins\n",
            "Train Loss: 0.24646826088428497\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1100 iterations: 4.045576886336009 mins\n",
            "Train Loss: 0.433912992477417\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1200 iterations: 4.4039594252904255 mins\n",
            "Train Loss: 0.21857178211212158\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1300 iterations: 4.7526849468549095 mins\n",
            "Train Loss: 0.2773437201976776\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1400 iterations: 5.100571032365163 mins\n",
            "Train Loss: 0.2363397479057312\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1500 iterations: 5.492413667837779 mins\n",
            "Train Loss: 0.39444249868392944\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1600 iterations: 5.8223614891370135 mins\n",
            "Train Loss: 0.38789671659469604\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1700 iterations: 6.155698458353679 mins\n",
            "Train Loss: 0.17854489386081696\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1800 iterations: 6.487609581152598 mins\n",
            "Train Loss: 0.5820400714874268\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1900 iterations: 6.820589546362559 mins\n",
            "Train Loss: 0.390639990568161\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2000 iterations: 7.1534465789794925 mins\n",
            "Train Loss: 0.7008284330368042\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 83.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2100 iterations: 7.4867545167605085 mins\n",
            "Train Loss: 0.18875037133693695\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 87.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2200 iterations: 7.818379946549734 mins\n",
            "Train Loss: 0.37333184480667114\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2300 iterations: 8.152580201625824 mins\n",
            "Train Loss: 0.3670193552970886\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2400 iterations: 8.48356438477834 mins\n",
            "Train Loss: 0.3596368432044983\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2500 iterations: 8.826465960343679 mins\n",
            "Train Loss: 0.15774163603782654\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2600 iterations: 9.159111694494884 mins\n",
            "Train Loss: 0.1537303924560547\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2700 iterations: 9.876695843537648 mins\n",
            "Train Loss: 0.1437603235244751\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2800 iterations: 10.208904266357422 mins\n",
            "Train Loss: 0.13839948177337646\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2900 iterations: 10.542962106068929 mins\n",
            "Train Loss: 0.1324675977230072\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3000 iterations: 10.877641121546427 mins\n",
            "Train Loss: 0.32088226079940796\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3100 iterations: 11.210184129079183 mins\n",
            "Train Loss: 0.15884995460510254\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3200 iterations: 11.5408456047376 mins\n",
            "Train Loss: 0.30955177545547485\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3300 iterations: 11.871591254075367 mins\n",
            "Train Loss: 0.11544559895992279\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3400 iterations: 12.205634311834972 mins\n",
            "Train Loss: 0.2986716032028198\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3500 iterations: 12.535493421554566 mins\n",
            "Train Loss: 0.2935112416744232\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3600 iterations: 12.865248735745748 mins\n",
            "Train Loss: 0.09917939454317093\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3700 iterations: 13.240724956989288 mins\n",
            "Train Loss: 0.28459829092025757\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 81.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3800 iterations: 13.712538735071819 mins\n",
            "Train Loss: 0.28031396865844727\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3900 iterations: 14.0496501882871 mins\n",
            "Train Loss: 0.2763846516609192\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4000 iterations: 14.379491881529491 mins\n",
            "Train Loss: 0.1211187019944191\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 98.0% 9 way one-shot learning accuracy \n",
            "\n",
            "Current best: 98.0, previous best: 93.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4100 iterations: 14.709054990609486 mins\n",
            "Train Loss: 0.2797790765762329\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4200 iterations: 15.03945151567459 mins\n",
            "Train Loss: 0.27595922350883484\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4300 iterations: 15.368236339092254 mins\n",
            "Train Loss: 0.2714999318122864\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4400 iterations: 15.69680290222168 mins\n",
            "Train Loss: 0.26671063899993896\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4500 iterations: 16.026795927683512 mins\n",
            "Train Loss: 0.07934922724962234\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4600 iterations: 16.355745073159536 mins\n",
            "Train Loss: 0.2578998804092407\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4700 iterations: 16.68571525812149 mins\n",
            "Train Loss: 0.07278109341859818\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4800 iterations: 17.419721829891206 mins\n",
            "Train Loss: 0.07074917107820511\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 81.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4900 iterations: 17.755887536207833 mins\n",
            "Train Loss: 0.24722164869308472\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.0% 9 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5000 iterations: 18.087249553203584 mins\n",
            "Train Loss: 0.24339309334754944\n",
            "Evaluating model on 100 random 9 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 9 way one-shot learning accuracy \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7wtzP-wxQli",
        "colab_type": "text"
      },
      "source": [
        "##Thus we got an average accuracy of 70%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAa3Fk61Eg0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPhrKvsEg-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}